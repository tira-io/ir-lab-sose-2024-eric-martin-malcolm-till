{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Baseline Retrieval System\n",
    "\n",
    "This jupyter notebook serves as baseline retrieval system that you can try to improve upon.\n",
    "We will use the a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This serves Jupyter notebook only serves as retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine.\n",
    "\n",
    "Building your own index can be already one way that you can try to improve upon this baseline (if you want to focus on creating good document representations). Other ways could include reformulating queries or tuning parameters or building better retrieval pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tira\n",
      "  Using cached tira-0.0.134-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting ir-datasets\n",
      "  Using cached ir_datasets-0.5.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting python-terrier\n",
      "  Using cached python_terrier-0.10.1-py3-none-any.whl\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.3.1-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting requests==2.*,>=2.26 (from tira)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting docker==7.*,>=7.1.0 (from tira)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting numpy==1.* (from tira)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.8/114.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from tira)\n",
      "  Downloading pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from tira) (24.1)\n",
      "Collecting tqdm (from tira)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting urllib3>=1.26.0 (from docker==7.*,>=7.1.0->tira)\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests==2.*,>=2.26->tira)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests==2.*,>=2.26->tira)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests==2.*,>=2.26->tira)\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting beautifulsoup4>=4.4.1 (from ir-datasets)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting inscriptis>=2.2.0 (from ir-datasets)\n",
      "  Using cached inscriptis-2.5.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting lxml>=4.5.2 (from ir-datasets)\n",
      "  Using cached lxml-5.2.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Collecting pyyaml>=5.3.1 (from ir-datasets)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting trec-car-tools>=2.5.4 (from ir-datasets)\n",
      "  Using cached trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
      "Collecting lz4>=3.1.10 (from ir-datasets)\n",
      "  Using cached lz4-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Collecting warc3-wet>=0.2.3 (from ir-datasets)\n",
      "  Using cached warc3_wet-0.2.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets)\n",
      "  Using cached warc3_wet_clueweb09-0.2.5-py3-none-any.whl\n",
      "Collecting zlib-state>=0.1.3 (from ir-datasets)\n",
      "  Using cached zlib_state-0.1.6-cp311-cp311-macosx_13_0_arm64.whl\n",
      "Collecting ijson>=3.1.3 (from ir-datasets)\n",
      "  Using cached ijson-3.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (21 kB)\n",
      "Collecting unlzw3>=0.2.1 (from ir-datasets)\n",
      "  Using cached unlzw3-0.2.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting wget (from python-terrier)\n",
      "  Using cached wget-3.2-py3-none-any.whl\n",
      "Collecting pyjnius>=1.4.2 (from python-terrier)\n",
      "  Using cached pyjnius-1.6.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (10 kB)\n",
      "Collecting matchpy (from python-terrier)\n",
      "  Using cached matchpy-0.5.5-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated (from python-terrier)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting chest (from python-terrier)\n",
      "  Using cached chest-0.2.3-py3-none-any.whl\n",
      "Collecting scipy (from python-terrier)\n",
      "  Using cached scipy-1.14.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting joblib (from python-terrier)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting nptyping==1.4.4 (from python-terrier)\n",
      "  Using cached nptyping-1.4.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting more-itertools (from python-terrier)\n",
      "  Using cached more_itertools-10.3.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jinja2 (from python-terrier)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting statsmodels (from python-terrier)\n",
      "  Using cached statsmodels-0.14.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting ir-measures>=0.3.1 (from python-terrier)\n",
      "  Using cached ir_measures-0.3.3-py3-none-any.whl\n",
      "Collecting dill (from python-terrier)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytrec-eval-terrier>=0.5.3 (from python-terrier)\n",
      "  Using cached pytrec_eval_terrier-0.5.6-cp311-cp311-macosx_10_9_universal2.whl.metadata (777 bytes)\n",
      "Collecting typish>=1.7.0 (from nptyping==1.4.4->python-terrier)\n",
      "  Using cached typish-1.9.3-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.4.1->ir-datasets)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting cwl-eval>=1.0.10 (from ir-measures>=0.3.1->python-terrier)\n",
      "  Using cached cwl_eval-1.0.12-py3-none-any.whl\n",
      "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets)\n",
      "  Using cached cbor-1.0.0-cp311-cp311-macosx_13_0_arm64.whl\n",
      "Collecting heapdict (from chest->python-terrier)\n",
      "  Using cached HeapDict-1.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated->python-terrier)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->python-terrier)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting multiset<3.0,>=2.0 (from matchpy->python-terrier)\n",
      "  Using cached multiset-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->tira) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->tira)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->tira)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting patsy>=0.5.6 (from statsmodels->python-terrier)\n",
      "  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.11/site-packages (from patsy>=0.5.6->statsmodels->python-terrier) (1.16.0)\n",
      "Using cached tira-0.0.134-py3-none-any.whl (92 kB)\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached ir_datasets-0.5.8-py3-none-any.whl (347 kB)\n",
      "Using cached nptyping-1.4.4-py3-none-any.whl (31 kB)\n",
      "Using cached transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "Using cached torch-2.3.1-cp311-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached ijson-3.3.0-cp311-cp311-macosx_11_0_arm64.whl (57 kB)\n",
      "Using cached inscriptis-2.5.0-py3-none-any.whl (45 kB)\n",
      "Using cached lxml-5.2.2-cp311-cp311-macosx_10_9_universal2.whl (8.1 MB)\n",
      "Using cached lz4-4.3.3-cp311-cp311-macosx_11_0_arm64.whl (212 kB)\n",
      "Using cached pyjnius-1.6.1-cp311-cp311-macosx_10_9_universal2.whl (514 kB)\n",
      "Using cached pytrec_eval_terrier-0.5.6-cp311-cp311-macosx_10_9_universal2.whl (138 kB)\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.5/167.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl (278 kB)\n",
      "Using cached safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl (410 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Using cached unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
      "Using cached warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached matchpy-0.5.5-py3-none-any.whl (69 kB)\n",
      "Using cached more_itertools-10.3.0-py3-none-any.whl (59 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Downloading pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.14.0-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Using cached statsmodels-0.14.2-cp311-cp311-macosx_11_0_arm64.whl (10.1 MB)\n",
      "Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multiset-2.1.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Using cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using cached typish-1.9.3-py3-none-any.whl (45 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Installing collected packages: wget, warc3-wet-clueweb09, warc3-wet, typish, pytz, pyjnius, multiset, mpmath, ijson, heapdict, cbor, zlib-state, wrapt, urllib3, unlzw3, tzdata, tqdm, sympy, soupsieve, safetensors, regex, pyyaml, pytrec-eval-terrier, numpy, networkx, more-itertools, matchpy, MarkupSafe, lz4, lxml, joblib, idna, fsspec, filelock, dill, click, chest, charset-normalizer, certifi, trec-car-tools, scipy, requests, patsy, pandas, nptyping, nltk, jinja2, deprecated, cwl-eval, beautifulsoup4, torch, statsmodels, ir-measures, inscriptis, huggingface-hub, docker, tokenizers, tira, ir-datasets, transformers, python-terrier\n",
      "Successfully installed MarkupSafe-2.1.5 beautifulsoup4-4.12.3 cbor-1.0.0 certifi-2024.6.2 charset-normalizer-3.3.2 chest-0.2.3 click-8.1.7 cwl-eval-1.0.12 deprecated-1.2.14 dill-0.3.8 docker-7.1.0 filelock-3.15.4 fsspec-2024.6.1 heapdict-1.0.1 huggingface-hub-0.23.4 idna-3.7 ijson-3.3.0 inscriptis-2.5.0 ir-datasets-0.5.8 ir-measures-0.3.3 jinja2-3.1.4 joblib-1.4.2 lxml-5.2.2 lz4-4.3.3 matchpy-0.5.5 more-itertools-10.3.0 mpmath-1.3.0 multiset-2.1.1 networkx-3.3 nltk-3.8.1 nptyping-1.4.4 numpy-1.26.4 pandas-2.2.2 patsy-0.5.6 pyjnius-1.6.1 python-terrier-0.10.1 pytrec-eval-terrier-0.5.6 pytz-2024.1 pyyaml-6.0.1 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 scipy-1.14.0 soupsieve-2.5 statsmodels-0.14.2 sympy-1.12.1 tira-0.0.134 tokenizers-0.19.1 torch-2.3.1 tqdm-4.66.4 transformers-4.42.3 trec-car-tools-2.6 typish-1.9.3 tzdata-2024.1 unlzw3-0.2.2 urllib3-2.2.2 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 wget-3.2 wrapt-1.16.0 zlib-state-0.1.6\n"
     ]
    }
   ],
   "source": [
    "# You only need to execute this cell if you are using Google Golab.\n",
    "# If you use GitHub Codespaces, everything is already installed.\n",
    "!pip3 install tira ir-datasets python-terrier transformers torch nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinschlenk/Desktop/IRS-combined/combined/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/martinschlenk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.1 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index\n",
    "\n",
    "The type of the index object that we load is `<class 'jnius.reflect.org.terrier.structures.Index'>`, in fact a [Java class](http://terrier.org/docs/v3.6/javadoc/org/terrier/structures/Index.html) wrapped into Python. However, you do not need to worry about this: at this point, we will simply use the provided Index object to run procedures defined in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline\n",
    "\n",
    "We will define a BM25 retrieval pipeline as baseline. For details, see:\n",
    "\n",
    "- [https://pyterrier.readthedocs.io](https://pyterrier.readthedocs.io)\n",
    "- [https://github.com/terrier-org/ecir2021tutorial](https://github.com/terrier-org/ecir2021tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we have a short look at the first three topics:\n",
      "  qid                                     query\n",
      "0   1  retrieval system improving effectiveness\n",
      "1   2  machine learning language identification\n",
      "2   3             social media detect self harm\n"
     ]
    }
   ],
   "source": [
    "print('First, we have a short look at the first three topics:')\n",
    "\n",
    "topics = pt_dataset.get_topics('text')\n",
    "print(topics.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting the queries...\n",
      "  qid                                     query\n",
      "0   1  retrieval system improving effectiveness\n",
      "1   2                          machine learning\n",
      "2   3             social media detect self harm\n"
     ]
    }
   ],
   "source": [
    "#Query Segmentation\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# def advanced_segment_query(query):\n",
    "#     ner_results = nlp(query)\n",
    "#     segments = [result['word'] for result in ner_results if result['entity'] in ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']]\n",
    "#     if not segments:\n",
    "#         segments = word_tokenize(query)\n",
    "#     return \" \".join(segments)\n",
    "\n",
    "domain_specific_terms = [\n",
    "    \"natural language processing\", \"NLP\", \"information retrieval\", \"IR\",\n",
    "    \"machine learning\", \"deep learning\", \"neural network\", \"text mining\",\n",
    "    \"language model\", \"BERT\", \"transformer\", \"word embeddings\", \"semantic search\",\n",
    "    \"question answering\", \"text classification\", \"entity recognition\",\n",
    "    \"tokenization\", \"part-of-speech tagging\", \"POS tagging\", \"named entity recognition\", \"NER\",\n",
    "    \"sentiment analysis\", \"topic modeling\", \"latent Dirichlet allocation\", \"LDA\",\n",
    "    \"vector space model\", \"TF-IDF\", \"BM25\", \"relevance feedback\",\n",
    "    \"information retrieval evaluation\", \"precision\", \"recall\", \"F1 score\",\n",
    "    \"mean average precision\", \"MAP\", \"normalized discounted cumulative gain\", \"nDCG\",\n",
    "    \"word2vec\", \"GloVe\", \"fastText\", \"attention mechanism\",\n",
    "    \"sequence-to-sequence\", \"seq2seq\", \"encoder-decoder\", \"automatic summarization\",\n",
    "    \"machine translation\", \"language generation\", \"dialogue systems\", \"chatbots\",\n",
    "    \"cross-lingual information retrieval\", \"multilingual models\", \"transfer learning\",\n",
    "    \"fine-tuning\", \"pre-trained models\", \"zero-shot learning\",\n",
    "    \"few-shot learning\", \"domain adaptation\", \"semi-supervised learning\",\n",
    "    \"unsupervised learning\", \"self-supervised learning\", \"contrastive learning\",\n",
    "    \"contextual embeddings\", \"contextualized word representations\",\n",
    "    \"transformer-based models\", \"convolutional neural networks\", \"CNNs\",\n",
    "    \"recurrent neural networks\", \"RNNs\", \"long short-term memory\", \"LSTM\",\n",
    "    \"gated recurrent units\", \"GRU\", \"sequence labeling\", \"dependency parsing\",\n",
    "    \"constituency parsing\", \"syntactic parsing\", \"semantic parsing\",\n",
    "    \"coreference resolution\", \"relation extraction\", \"information extraction\",\n",
    "    \"knowledge graphs\", \"ontologies\", \"semantic role labeling\", \"SRL\",\n",
    "    \"document retrieval\", \"passage retrieval\", \"question answering systems\",\n",
    "    \"retrieval-augmented generation\", \"RAG\", \"open-domain QA\", \"closed-domain QA\",\n",
    "    \"query expansion\", \"query reformulation\", \"interactive information retrieval\",\n",
    "    \"user modeling\", \"personalized search\", \"context-aware retrieval\",\n",
    "    \"query understanding\", \"query intent\", \"search engine optimization\", \"SEO\",\n",
    "    \"click-through rate\", \"CTR\", \"session-based search\", \"search result diversification\",\n",
    "    \"exploratory search\", \"faceted search\", \"enterprise search\",\n",
    "    \"legal information retrieval\", \"medical information retrieval\",\n",
    "    \"scientific information retrieval\", \"scholarly search\", \"academic search\",\n",
    "    \"digital libraries\", \"citation analysis\", \"bibliometrics\", \"altmetrics\",\n",
    "    \"author disambiguation\", \"document clustering\", \"document classification\",\n",
    "    \"information visualization\", \"search interfaces\", \"human-computer interaction\",\n",
    "    \"HCI\", \"recommendation systems\", \"collaborative filtering\", \"content-based filtering\",\n",
    "    \"hybrid recommendation\", \"ranking algorithms\", \"learning to rank\", \"LTR\",\n",
    "    \"pairwise ranking\", \"listwise ranking\", \"pointwise ranking\", \"click models\",\n",
    "    \"user feedback\", \"implicit feedback\", \"explicit feedback\", \"active learning\",\n",
    "    \"crowdsourcing\", \"data annotation\", \"evaluation metrics\", \"benchmark datasets\"\n",
    "]\n",
    "def advanced_segment_query(query):\n",
    "    ner_results = nlp(query)\n",
    "    segments = set(result['word'] for result in ner_results if result['entity'] in ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'])\n",
    "    for term in domain_specific_terms:\n",
    "        if term in query:\n",
    "            segments.add(term)\n",
    "    if not segments:\n",
    "        segments = word_tokenize(query)\n",
    "    return \" \".join(segments)\n",
    "\n",
    "print('Segmenting the queries...')\n",
    "segmented_topics = topics.copy()\n",
    "segmented_topics['query'] = segmented_topics['query'].apply(advanced_segment_query)\n",
    "print(segmented_topics.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "Done. Here are the first 10 entries of the run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94858</td>\n",
       "      <td>2004.cikm_conference-2004.47</td>\n",
       "      <td>0</td>\n",
       "      <td>15.681777</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>125137</td>\n",
       "      <td>1989.ipm_journal-ir0volumeA25A4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>15.047380</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>125817</td>\n",
       "      <td>2005.ipm_journal-ir0volumeA41A5.11</td>\n",
       "      <td>2</td>\n",
       "      <td>14.144223</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5868</td>\n",
       "      <td>W05-0704</td>\n",
       "      <td>3</td>\n",
       "      <td>14.025748</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>84876</td>\n",
       "      <td>2016.ntcir_conference-2016.90</td>\n",
       "      <td>4</td>\n",
       "      <td>13.947994</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>82472</td>\n",
       "      <td>1998.sigirconf_conference-98.15</td>\n",
       "      <td>5</td>\n",
       "      <td>13.901647</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>94415</td>\n",
       "      <td>2008.cikm_conference-2008.183</td>\n",
       "      <td>6</td>\n",
       "      <td>13.808208</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>17496</td>\n",
       "      <td>O01-2005</td>\n",
       "      <td>7</td>\n",
       "      <td>13.749449</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>82490</td>\n",
       "      <td>1998.sigirconf_conference-98.33</td>\n",
       "      <td>8</td>\n",
       "      <td>13.735541</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>124801</td>\n",
       "      <td>2006.ipm_journal-ir0volumeA42A3.2</td>\n",
       "      <td>9</td>\n",
       "      <td>13.569263</td>\n",
       "      <td>retrieval system improving effectiveness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid                               docno  rank      score  \\\n",
       "0   1   94858        2004.cikm_conference-2004.47     0  15.681777   \n",
       "1   1  125137   1989.ipm_journal-ir0volumeA25A4.2     1  15.047380   \n",
       "2   1  125817  2005.ipm_journal-ir0volumeA41A5.11     2  14.144223   \n",
       "3   1    5868                            W05-0704     3  14.025748   \n",
       "4   1   84876       2016.ntcir_conference-2016.90     4  13.947994   \n",
       "5   1   82472     1998.sigirconf_conference-98.15     5  13.901647   \n",
       "6   1   94415       2008.cikm_conference-2008.183     6  13.808208   \n",
       "7   1   17496                            O01-2005     7  13.749449   \n",
       "8   1   82490     1998.sigirconf_conference-98.33     8  13.735541   \n",
       "9   1  124801   2006.ipm_journal-ir0volumeA42A3.2     9  13.569263   \n",
       "\n",
       "                                      query  \n",
       "0  retrieval system improving effectiveness  \n",
       "1  retrieval system improving effectiveness  \n",
       "2  retrieval system improving effectiveness  \n",
       "3  retrieval system improving effectiveness  \n",
       "4  retrieval system improving effectiveness  \n",
       "5  retrieval system improving effectiveness  \n",
       "6  retrieval system improving effectiveness  \n",
       "7  retrieval system improving effectiveness  \n",
       "8  retrieval system improving effectiveness  \n",
       "9  retrieval system improving effectiveness  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Now we do the retrieval...')\n",
    "run = bm25(segmented_topics)\n",
    "\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations\n",
    "\n",
    "The output of a prototypical retrieval system is a run file. This run file can later (optimally in a different notebook) be statistically evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs\".\n",
      "Done. run file is stored under \"../runs/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
