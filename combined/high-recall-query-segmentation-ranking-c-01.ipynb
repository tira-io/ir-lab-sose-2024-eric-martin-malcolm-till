{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Combined Retrieval System with Query Segmentation\n",
    "\n",
    "This jupyter notebook serves as an improved retrieval system combining BM25, Query Expansion, Query Segmentation, and additional reranking models.\n",
    "We will use a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This notebook serves as a retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "We will use [tira](https://www.tira.io/), an information retrieval shared task platform, for loading the (pre-built) retrieval index and [ir_dataset](https://ir-datasets.com/) to subsequently build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine. We'll also use NLTK for query segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ne_chunk\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Query Segmentation Function and the Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_query(query):\n",
    "    # Tokenize the query\n",
    "    tokens = word_tokenize(query)\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Perform named entity recognition\n",
    "    chunked = ne_chunk(tagged)\n",
    "    \n",
    "    # Extract named entities and other important phrases\n",
    "    segments = []\n",
    "    current_segment = []\n",
    "    for subtree in chunked:\n",
    "        if isinstance(subtree, nltk.Tree):\n",
    "            segments.append(' '.join(word for word, pos in subtree.leaves()))\n",
    "        else:\n",
    "            word, pos = subtree\n",
    "            if pos.startswith('NN') or pos.startswith('JJ'):\n",
    "                current_segment.append(word)\n",
    "            else:\n",
    "                if current_segment:\n",
    "                    segments.append(' '.join(current_segment))\n",
    "                    current_segment = []\n",
    "    if current_segment:\n",
    "        segments.append(' '.join(current_segment))\n",
    "    \n",
    "    # Join segments with quotes for exact phrase matching\n",
    "    segmented_query = ' '.join(f'\"{segment}\"' for segment in segments)\n",
    "    \n",
    "    return segmented_query\n",
    "\n",
    "# Query segmentation as a PyTerrier transformer\n",
    "query_segmentation = pt.apply.query(segment_query)\n",
    "\n",
    "# Base retrieval model with BM25\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "# Query expansion with Bo1\n",
    "bo1_expansion = pt.rewrite.Bo1QueryExpansion(index, fb_docs=10, fb_terms=20)\n",
    "bm25_bo1 = bm25 >> bo1_expansion >> bm25\n",
    "\n",
    "# Additional reranking models\n",
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "dirichletLM = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n",
    "\n",
    "# Combined retrieval pipeline with query segmentation\n",
    "combined_pipeline = query_segmentation >> (bm25_bo1 + 2 * tf_idf + 2 * dirichletLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First, we have a short look at the first three topics:')\n",
    "print(pt_dataset.get_topics('text').head(3))\n",
    "\n",
    "print('Now we do the retrieval...')\n",
    "run = combined_pipeline.transform(pt_dataset.get_topics('text'))\n",
    "\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "print(run.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'runs' directory if it doesn't exist\n",
    "os.makedirs('../runs', exist_ok=True)\n",
    "\n",
    "persist_and_normalize_run(run, system_name='combined-bm25-bo1-tfidf-dirichlet-with-segmentation', default_output='../runs')\n",
    "print('Run file is stored under \"../runs/run.txt\".')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
