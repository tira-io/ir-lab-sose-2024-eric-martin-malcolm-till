{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Combined Retrieval System\n",
    "\n",
    "This jupyter notebook serves as an improved retrieval system combining components from both provided notebooks.\n",
    "We will use a corpus of scientific papers (title + abstracts) from the fields of information retrieval and natural language processing (the [IR Anthology](https://ir.webis.de/anthology/) and the [ACL Anthology](https://aclanthology.org/)). This notebook serves as a retrieval system, i.e., it gets a set of information needs (topics) and a corpus as input and produces a run file as output. Please do evaluations in a new dedicated notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Install Required Packages and Setup Logging\n",
    "\n",
    "Execute this cell if you're using Google Colab or if you haven't installed these packages yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tira ir-datasets python-terrier transformers torch nltk\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(\"Logging initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize PyTerrier and TIRA client\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "logging.info(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "    pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "    logging.info(\"Dataset loaded successfully.\")\n",
    "\n",
    "    # A (pre-built) PyTerrier index loaded from TIRA\n",
    "    index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "    logging.info(\"Index loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred while loading the dataset or index: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base retrieval model with BM25\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "# Query expansion with Bo1\n",
    "# fb_docs: number of feedback documents, fb_terms: number of expansion terms\n",
    "bo1_expansion = pt.rewrite.Bo1QueryExpansion(index, fb_docs=10, fb_terms=20)\n",
    "bm25_bo1 = bm25 >> bo1_expansion >> bm25\n",
    "\n",
    "# Additional reranking models\n",
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "dirichletLM = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n",
    "\n",
    "# Combined retrieval pipeline\n",
    "# We're giving more weight to TF-IDF and DirichletLM models\n",
    "combined_pipeline = bm25_bo1 + 2 * tf_idf + 2 * dirichletLM\n",
    "\n",
    "logging.info(\"Retrieval pipeline defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First, we have a short look at the first three topics:')\n",
    "topics = pt_dataset.get_topics('text')\n",
    "print(topics.head(3))\n",
    "\n",
    "# Query Segmentation\n",
    "try:\n",
    "    print('\\nInitializing BERT model for Named Entity Recognition...')\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "    model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    logging.info('BERT model loaded successfully')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading BERT model: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Domain-specific terms for query segmentation\n",
    "domain_specific_terms = [\n",
    "    \"natural language processing\", \"NLP\", \"information retrieval\", \"IR\",\n",
    "    \"machine learning\", \"deep learning\", \"neural network\", \"text mining\",\n",
    "    \"language model\", \"BERT\", \"transformer\", \"word embeddings\", \"semantic search\",\n",
    "    \"question answering\", \"text classification\", \"entity recognition\",\n",
    "    \"tokenization\", \"part-of-speech tagging\", \"POS tagging\", \"named entity recognition\", \"NER\",\n",
    "    \"sentiment analysis\", \"topic modeling\", \"latent Dirichlet allocation\", \"LDA\",\n",
    "    \"vector space model\", \"TF-IDF\", \"BM25\", \"relevance feedback\",\n",
    "    \"information retrieval evaluation\", \"precision\", \"recall\", \"F1 score\",\n",
    "    \"mean average precision\", \"MAP\", \"normalized discounted cumulative gain\", \"nDCG\",\n",
    "    \"word2vec\", \"GloVe\", \"fastText\", \"attention mechanism\",\n",
    "    \"sequence-to-sequence\", \"seq2seq\", \"encoder-decoder\", \"automatic summarization\",\n",
    "    \"machine translation\", \"language generation\", \"dialogue systems\", \"chatbots\",\n",
    "    \"cross-lingual information retrieval\", \"multilingual models\", \"transfer learning\",\n",
    "    \"fine-tuning\", \"pre-trained models\", \"zero-shot learning\",\n",
    "    \"few-shot learning\", \"domain adaptation\", \"semi-supervised learning\",\n",
    "    \"unsupervised learning\", \"self-supervised learning\", \"contrastive learning\",\n",
    "    \"contextual embeddings\", \"contextualized word representations\",\n",
    "    \"transformer-based models\", \"convolutional neural networks\", \"CNNs\",\n",
    "    \"recurrent neural networks\", \"RNNs\", \"long short-term memory\", \"LSTM\",\n",
    "    \"gated recurrent units\", \"GRU\", \"sequence labeling\", \"dependency parsing\",\n",
    "    \"constituency parsing\", \"syntactic parsing\", \"semantic parsing\",\n",
    "    \"coreference resolution\", \"relation extraction\", \"information extraction\",\n",
    "    \"knowledge graphs\", \"ontologies\", \"semantic role labeling\", \"SRL\",\n",
    "    \"document retrieval\", \"passage retrieval\", \"question answering systems\",\n",
    "    \"retrieval-augmented generation\", \"RAG\", \"open-domain QA\", \"closed-domain QA\",\n",
    "    \"query expansion\", \"query reformulation\", \"interactive information retrieval\",\n",
    "    \"user modeling\", \"personalized search\", \"context-aware retrieval\",\n",
    "    \"query understanding\", \"query intent\", \"search engine optimization\", \"SEO\",\n",
    "    \"click-through rate\", \"CTR\", \"session-based search\", \"search result diversification\",\n",
    "    \"exploratory search\", \"faceted search\", \"enterprise search\",\n",
    "    \"legal information retrieval\", \"medical information retrieval\",\n",
    "    \"scientific information retrieval\", \"scholarly search\", \"academic search\",\n",
    "    \"digital libraries\", \"citation analysis\", \"bibliometrics\", \"altmetrics\",\n",
    "    \"author disambiguation\", \"document clustering\", \"document classification\",\n",
    "    \"information visualization\", \"search interfaces\", \"human-computer interaction\",\n",
    "    \"HCI\", \"recommendation systems\", \"collaborative filtering\", \"content-based filtering\",\n",
    "    \"hybrid recommendation\", \"ranking algorithms\", \"learning to rank\", \"LTR\",\n",
    "    \"pairwise ranking\", \"listwise ranking\", \"pointwise ranking\", \"click models\",\n",
    "    \"user feedback\", \"implicit feedback\", \"explicit feedback\", \"active learning\",\n",
    "    \"crowdsourcing\", \"data annotation\", \"evaluation metrics\", \"benchmark datasets\"\n",
    "]\n",
    "\n",
    "def advanced_segment_query(query):\n",
    "    ner_results = nlp(query)\n",
    "    segments = set(result['word'] for result in ner_results if result['entity'] in ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'])\n",
    "    for term in domain_specific_terms:\n",
    "        if term.lower() in query.lower():  # Case-insensitive matching\n",
    "            segments.add(term)\n",
    "    if not segments:\n",
    "        segments = word_tokenize(query)\n",
    "    return \" \".join(segments)\n",
    "\n",
    "print('\\nSegmenting the queries...')\n",
    "segmented_topics = topics.copy()\n",
    "segmented_topics['query'] = segmented_topics['query'].apply(advanced_segment_query)\n",
    "print(segmented_topics.head(3))\n",
    "\n",
    "print('\\nNow we do the retrieval...')\n",
    "run = combined_pipeline.transform(segmented_topics)\n",
    "\n",
    "print('\\nDone. Here are the first 10 entries of the run')\n",
    "print(run.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Persist the run file for subsequent evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('../runs', exist_ok=True)\n",
    "    persist_and_normalize_run(run, system_name='combined-bm25-bo1-tfidf-dirichlet', default_output='../runs')\n",
    "    output_dir = os.environ.get('outputDir', '/output')\n",
    "    run.to_csv(os.path.join(output_dir, 'run.txt'), sep='\\t', index=False, header=False)\n",
    "    logging.info(f\"Results saved to {os.path.join(output_dir, 'run.txt')}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred while saving the run file: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
