import pyterrier as pt
from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run
from tira.rest_api_client import Client
import os

# Initialize PyTerrier
if not pt.started():
    pt.init()

# Initialize TIRA client
tira = Client()

# Set dataset and index names
dataset_name = 'ir-lab-sose-2024/ir-acl-anthology-20240504-training'
index_name = 'ir-lab-sose-2024/tira-ir-starter'

try:
    # Use tira.pt to get the dataset
    pt_dataset = tira.pt.dataset(dataset_name)
    print("Dataset loaded successfully:")
    print(f"Number of topics: {len(pt_dataset.get_topics())}")
    print(f"Number of qrels: {len(pt_dataset.get_qrels())}")
except Exception as e:
    print(f"Error loading dataset: {e}")
    pt_dataset = None

try:
    # Use tira.pt to get the index
    index = tira.pt.index(index_name, pt_dataset)
    print("Index loaded successfully:")
    print(f"Number of documents: {index.getCollectionStatistics().getNumberOfDocuments()}")
except Exception as e:
    print(f"Error loading index: {e}")
    index = None

# If we have both the dataset and the index, we can proceed with the retrieval system
if pt_dataset is not None and index is not None:
    # Define retrieval components
    bm25 = pt.BatchRetrieve(index, wmodel="BM25")
    bo1_expansion = pt.rewrite.Bo1QueryExpansion(index, fb_docs=10, fb_terms=20)

    # Set up ColBERT
    import pyterrier_colbert.indexing
    from pyterrier_colbert.ranking import ColBERTFactory

    checkpoint = "http://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/colbert_model_checkpoint.zip"
    colbert_indexer = pyterrier_colbert.indexing.ColBERTIndexer(checkpoint=checkpoint, 
                                                                index_root="/content",
                                                                index_name="colbert_index",
                                                                chunksize=3)
    colbert_indexer.index(pt_dataset.get_corpus_iter())

    colbert_factory = ColBERTFactory(checkpoint, "/content/colbert_index")
    colbert = colbert_factory.text_scorer()

    # Create combined retrieval pipeline
    combined_pipeline = (
        bm25 
        >> pt.text.get_text(pt_dataset, 'text') 
        >> bo1_expansion 
        >> bm25 
        >> pt.text.get_text(pt_dataset, 'text') 
        >> colbert
    )

    # Create and persist the run
    topics = pt_dataset.get_topics()
    run = combined_pipeline(topics)

    persist_and_normalize_run(run, system_name='combined-bm25-bo1-colbert', default_output='../runs')
    print("Run file has been created and stored in '../runs' directory.")
else:
    print("Failed to load dataset or index. Cannot proceed with retrieval system.")