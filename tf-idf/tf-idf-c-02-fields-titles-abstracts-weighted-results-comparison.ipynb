{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024: Vergleichendes Retrieval System\n",
    "\n",
    "Dieses Jupyter Notebook implementiert und vergleicht mehrere Retrieval-Ansätze:\n",
    "1. Nur BM25\n",
    "2. BM25 + TF-IDF\n",
    "3. BM25 + Gewichtetes TF-IDF (mit Feldgewichtung)\n",
    "\n",
    "Wir verwenden ein Korpus wissenschaftlicher Arbeiten (Titel + Abstracts) aus den Bereichen Information Retrieval und Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1: Bibliotheken importieren und Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTerrier 0.10.1 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "from pyterrier.pipelines import *\n",
    "import pandas as pd\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "# Das Dataset und der Index\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2: Retrieval Pipelines definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Nur BM25\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "# 2. BM25 + TF-IDF\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "bm25_tfidf = bm25 >> tfidf\n",
    "\n",
    "# 3. BM25 + Gewichtetes TF-IDF\n",
    "fields = [\"title\", \"abstract\"]\n",
    "weights = [2.0, 1.0]  # Titel hat doppeltes Gewicht des Abstracts\n",
    "weighted_tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\", controls={\"c\": 1.0}, fields=fields)\n",
    "for field, weight in zip(fields, weights):\n",
    "    weighted_tfidf.controls[f\"w.{field}\"] = weight\n",
    "bm25_weighted_tfidf = bm25 >> weighted_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 3: Retrieval durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die ersten drei Topics:\n",
      "  qid                                     query\n",
      "0   1  retrieval system improving effectiveness\n",
      "1   2  machine learning language identification\n",
      "2   3             social media detect self harm\n",
      "\n",
      "Führe Retrieval für alle Ansätze durch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(TF_IDF): 100%|██████████| 68/68 [00:01<00:00, 62.02q/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "topics = pt_dataset.get_topics('text')\n",
    "print('Die ersten drei Topics:')\n",
    "print(topics.head(3))\n",
    "\n",
    "print('\\nFühre Retrieval für alle Ansätze durch...')\n",
    "run_bm25 = bm25(topics)\n",
    "run_bm25_tfidf = bm25_tfidf(topics)\n",
    "run_bm25_weighted_tfidf = bm25_weighted_tfidf(topics)\n",
    "print('Retrieval abgeschlossen.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 4: Ergebnisse vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vergleich der Top 10 Ergebnisse für die erste Anfrage (qid=1):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Requested level (qid) does not match index name (None)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m comparison \u001b[38;5;241m=\u001b[39m compare_top_k(runs)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVergleich der Top 10 Ergebnisse für die erste Anfrage (qid=1):\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(comparison[\u001b[43mcomparison\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Berechnen Sie den Overlap zwischen den Top-K Ergebnissen\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_overlap\u001b[39m(runs, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:2102\u001b[0m, in \u001b[0;36mIndex._get_level_values\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_level_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, level) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;124;03m    Return an Index of values for requested level.\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[38;5;124;03m    Index(['a', 'b', 'c'], dtype='object')\u001b[39;00m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_index_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:2012\u001b[0m, in \u001b[0;36mIndex._validate_index_level\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m   2009\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many levels: Index has only 1 level, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2010\u001b[0m         )\n\u001b[1;32m   2011\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m level \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname:\n\u001b[0;32m-> 2012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   2013\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested level (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match index name (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2014\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Requested level (qid) does not match index name (None)'"
     ]
    }
   ],
   "source": [
    "def compare_top_k(runs, k=10):\n",
    "    results = {}\n",
    "    for name, run in runs.items():\n",
    "        results[name] = run.groupby('qid').head(k)\n",
    "    \n",
    "    comparison = pd.DataFrame()\n",
    "    for name, result in results.items():\n",
    "        comparison[f'{name}_docno'] = result['docno']\n",
    "        comparison[f'{name}_score'] = result['score']\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "runs = {\n",
    "    'BM25': run_bm25,\n",
    "    'BM25+TF-IDF': run_bm25_tfidf,\n",
    "    'BM25+Weighted-TF-IDF': run_bm25_weighted_tfidf\n",
    "}\n",
    "\n",
    "comparison = compare_top_k(runs)\n",
    "print(f'Vergleich der Top 10 Ergebnisse für die erste Anfrage (qid=1):')\n",
    "print(comparison[comparison.index.get_level_values('qid') == '1'])\n",
    "\n",
    "# Berechnen Sie den Overlap zwischen den Top-K Ergebnissen\n",
    "def calculate_overlap(runs, k=10):\n",
    "    overlaps = {}\n",
    "    for name1, run1 in runs.items():\n",
    "        for name2, run2 in runs.items():\n",
    "            if name1 < name2:  # Vermeiden Sie doppelte Vergleiche\n",
    "                overlap = len(set(run1.groupby('qid').head(k)['docno']) & \n",
    "                              set(run2.groupby('qid').head(k)['docno']))\n",
    "                overlaps[f'{name1} vs {name2}'] = overlap\n",
    "    return overlaps\n",
    "\n",
    "overlaps = calculate_overlap(runs)\n",
    "print(f'\\nÜberlappung der Top 10 Ergebnisse zwischen den Ansätzen:')\n",
    "for comparison, overlap in overlaps.items():\n",
    "    print(f'{comparison}: {overlap} von 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 5: Run-Dateien persistieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_and_normalize_run(run_bm25, system_name='bm25-baseline', default_output='../runs')\n",
    "persist_and_normalize_run(run_bm25_tfidf, system_name='bm25-tfidf-combined', default_output='../runs')\n",
    "persist_and_normalize_run(run_bm25_weighted_tfidf, system_name='bm25-weighted-tfidf-combined', default_output='../runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 6: Analyse und Interpretation\n",
    "\n",
    "1. Betrachten Sie die Top-10-Ergebnisse für verschiedene Anfragen. Wie unterscheiden sich die Ergebnisse zwischen den Ansätzen?\n",
    "\n",
    "2. Analysieren Sie die Überlappung der Ergebnisse. Ein hoher Überlappungsgrad deutet darauf hin, dass die Ansätze ähnliche Ergebnisse liefern, während ein niedriger Grad auf unterschiedliche Stärken der Ansätze hinweisen könnte.\n",
    "\n",
    "3. Untersuchen Sie spezifische Fälle, in denen die gewichtete TF-IDF-Methode andere Ergebnisse liefert als die anderen Methoden. Sind diese Unterschiede auf die Titelgewichtung zurückzuführen?\n",
    "\n",
    "4. Betrachten Sie die Scores der verschiedenen Methoden. Wie unterscheiden sie sich in ihrer Verteilung und Größenordnung?\n",
    "\n",
    "5. Für eine gründlichere Evaluation wären Relevanzurteile erforderlich, um Metriken wie MAP oder NDCG zu berechnen. In Ermangelung solcher Urteile könnten Sie eine manuelle Stichprobenprüfung der Ergebnisse für einige ausgewählte Anfragen durchführen.\n",
    "\n",
    "6. Überlegen Sie, wie Sie die Ansätze weiter verbessern könnten. Mögliche Richtungen umfassen:\n",
    "   - Experimentieren mit verschiedenen Feldgewichten\n",
    "   - Einbeziehung zusätzlicher Felder (z.B. Autorennamen, Veröffentlichungsjahr)\n",
    "   - Implementierung von Query Expansion oder Pseudo-Relevanz-Feedback\n",
    "   - Verwendung von neueren Ranking-Modellen wie BERT oder andere Transformer-basierte Modelle\n",
    "\n",
    "Denken Sie daran, dass die 'beste' Methode oft von der spezifischen Aufgabe und dem Datensatz abhängt. Eine gründliche Evaluation und Analyse ist entscheidend, um zu verstehen, welcher Ansatz für Ihre spezifischen Bedürfnisse am besten geeignet ist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
